{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fb6ae06-6267-405e-92b2-89ff0f747fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadd96b-d673-4283-bcb6-a93f5281d7d6",
   "metadata": {},
   "source": [
    "## Predicting Fetal Health with Tree-Based Models \n",
    "\n",
    "## Introduction + Goal\n",
    "\n",
    "A cardiotocogram (CTG) is a medical test that records a fetus’s heart rate and the uterine contractions of the mother. Features extracted from a CTG often include things like baseline heart rate, variability, accelerations, and decelerations. Doctors use CTG data to assess fetal well-being. The CTG data, along with the doctor’s assessment of the fetus’s health, can also be used to train predictive models for fetal health classification.\n",
    "\n",
    "The goal of this project is to train three classification models to predict whether a fetus’s health is normal, suspect or pathological based on CTG data. Model performance will be evaluated and the models compared based on accuracy, F1-score, and interpretability. All three classification methods used will be tree-based: a simple Decision Tree, AdaBoost, and Random Forest. A Decision Tree is particularly useful in this case because it can capture non-linear relationships among the cardiotocogram features and provide an interpretable structure for understanding the predictions.\n",
    "\n",
    "## Data Description + Preprocessing\n",
    "\n",
    "The features used for model training were automatically extracted from cardiotocogram readings. Expert obstetricians also evaluated each CTG and classified the fetus’s health as normal, suspect, or pathological. Each of the 2126 rows represents a CTG for a mother–fetus pair. 21 columns contain  features extracted from the CTG and the 22nd column contains the experts’ classifications.\n",
    "\n",
    "Learn more about the dataset [here](https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1b98b36-571d-4aef-b5e1-6d404160fcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline value</th>\n",
       "      <th>accelerations</th>\n",
       "      <th>fetal_movement</th>\n",
       "      <th>uterine_contractions</th>\n",
       "      <th>light_decelerations</th>\n",
       "      <th>severe_decelerations</th>\n",
       "      <th>prolongued_decelerations</th>\n",
       "      <th>abnormal_short_term_variability</th>\n",
       "      <th>mean_value_of_short_term_variability</th>\n",
       "      <th>percentage_of_time_with_abnormal_long_term_variability</th>\n",
       "      <th>...</th>\n",
       "      <th>histogram_min</th>\n",
       "      <th>histogram_max</th>\n",
       "      <th>histogram_number_of_peaks</th>\n",
       "      <th>histogram_number_of_zeroes</th>\n",
       "      <th>histogram_mode</th>\n",
       "      <th>histogram_mean</th>\n",
       "      <th>histogram_median</th>\n",
       "      <th>histogram_variance</th>\n",
       "      <th>histogram_tendency</th>\n",
       "      <th>fetal_health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   baseline value  accelerations  fetal_movement  uterine_contractions  \\\n",
       "0           120.0          0.000             0.0                 0.000   \n",
       "1           132.0          0.006             0.0                 0.006   \n",
       "2           133.0          0.003             0.0                 0.008   \n",
       "3           134.0          0.003             0.0                 0.008   \n",
       "4           132.0          0.007             0.0                 0.008   \n",
       "\n",
       "   light_decelerations  severe_decelerations  prolongued_decelerations  \\\n",
       "0                0.000                   0.0                       0.0   \n",
       "1                0.003                   0.0                       0.0   \n",
       "2                0.003                   0.0                       0.0   \n",
       "3                0.003                   0.0                       0.0   \n",
       "4                0.000                   0.0                       0.0   \n",
       "\n",
       "   abnormal_short_term_variability  mean_value_of_short_term_variability  \\\n",
       "0                             73.0                                   0.5   \n",
       "1                             17.0                                   2.1   \n",
       "2                             16.0                                   2.1   \n",
       "3                             16.0                                   2.4   \n",
       "4                             16.0                                   2.4   \n",
       "\n",
       "   percentage_of_time_with_abnormal_long_term_variability  ...  histogram_min  \\\n",
       "0                                               43.0       ...           62.0   \n",
       "1                                                0.0       ...           68.0   \n",
       "2                                                0.0       ...           68.0   \n",
       "3                                                0.0       ...           53.0   \n",
       "4                                                0.0       ...           53.0   \n",
       "\n",
       "   histogram_max  histogram_number_of_peaks  histogram_number_of_zeroes  \\\n",
       "0          126.0                        2.0                         0.0   \n",
       "1          198.0                        6.0                         1.0   \n",
       "2          198.0                        5.0                         1.0   \n",
       "3          170.0                       11.0                         0.0   \n",
       "4          170.0                        9.0                         0.0   \n",
       "\n",
       "   histogram_mode  histogram_mean  histogram_median  histogram_variance  \\\n",
       "0           120.0           137.0             121.0                73.0   \n",
       "1           141.0           136.0             140.0                12.0   \n",
       "2           141.0           135.0             138.0                13.0   \n",
       "3           137.0           134.0             137.0                13.0   \n",
       "4           137.0           136.0             138.0                11.0   \n",
       "\n",
       "   histogram_tendency  fetal_health  \n",
       "0                 1.0           2.0  \n",
       "1                 0.0           1.0  \n",
       "2                 0.0           1.0  \n",
       "3                 1.0           1.0  \n",
       "4                 1.0           1.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fetal_health.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71188817-46ef-4f08-a904-43c8a3399ad6",
   "metadata": {},
   "source": [
    "To prepare the data for model training, I'll first extract the column holding the experts' diagnoses and remove it from the training data. The health diagnostic is encoded as 1 for Healthy, 2 for Suspect, and 3 for Pathological. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ba411a3-688a-4078-9e1c-b08054b5fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['fetal_health'].astype(int) # extract expert classifications \n",
    "data = df.drop(['fetal_health'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de58cc75-2cb5-484f-87a2-81b85380720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2126 entries, 0 to 2125\n",
      "Data columns (total 21 columns):\n",
      " #   Column                                                  Non-Null Count  Dtype  \n",
      "---  ------                                                  --------------  -----  \n",
      " 0   baseline value                                          2126 non-null   float64\n",
      " 1   accelerations                                           2126 non-null   float64\n",
      " 2   fetal_movement                                          2126 non-null   float64\n",
      " 3   uterine_contractions                                    2126 non-null   float64\n",
      " 4   light_decelerations                                     2126 non-null   float64\n",
      " 5   severe_decelerations                                    2126 non-null   float64\n",
      " 6   prolongued_decelerations                                2126 non-null   float64\n",
      " 7   abnormal_short_term_variability                         2126 non-null   float64\n",
      " 8   mean_value_of_short_term_variability                    2126 non-null   float64\n",
      " 9   percentage_of_time_with_abnormal_long_term_variability  2126 non-null   float64\n",
      " 10  mean_value_of_long_term_variability                     2126 non-null   float64\n",
      " 11  histogram_width                                         2126 non-null   float64\n",
      " 12  histogram_min                                           2126 non-null   float64\n",
      " 13  histogram_max                                           2126 non-null   float64\n",
      " 14  histogram_number_of_peaks                               2126 non-null   float64\n",
      " 15  histogram_number_of_zeroes                              2126 non-null   float64\n",
      " 16  histogram_mode                                          2126 non-null   float64\n",
      " 17  histogram_mean                                          2126 non-null   float64\n",
      " 18  histogram_median                                        2126 non-null   float64\n",
      " 19  histogram_variance                                      2126 non-null   float64\n",
      " 20  histogram_tendency                                      2126 non-null   float64\n",
      "dtypes: float64(21)\n",
      "memory usage: 348.9 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbb030-5ece-48e4-9173-c79c23903687",
   "metadata": {},
   "source": [
    "Since there is no missing data, I'm going to go ahead and split the training data 80/20 into training and testing sets for validation purposes. I'm also going to stratify the split based on the true labels to ensure each set has a similar distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91e54569-5587-4706-8c4d-415c153e9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5d9df-8f28-4dc2-9d9a-d49aac285dc1",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "I'm going to build 3 classifiers using different algorithms: decision tree, AdaBoost, and random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5e758-e5bd-4e41-be3c-b8877ae575fe",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "A decision tree is a model that makes predictions by recursively splitting data on feature values. Each internal node represents a decision based on a feature (like whether a measurement exceeds a threshold) and each leaf node represents a predicted outcome. Before I can train the model, I'm going to tune its hyperparameters using cross-validation. Specifically, GridSearchCV performs cross-validation for each combination of hyperparameters and selects the best combination based on a performance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c76987a-d42d-4d1a-be60-ef08e73147f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [None, 3, 5, 8], # values to test \n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [2, 5, 10]}\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = dt,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,                 \n",
    "    scoring = 'f1_macro',  # performance metric \n",
    "    n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "865b9dbe-f25e-4377-a014-2bb6b506257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 1, min_samples_split = 20)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_preds = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a868f5-3a82-45be-bf43-fd094da5edaf",
   "metadata": {},
   "source": [
    "### AdaBoost \n",
    "\n",
    "Boosting is a technique to improve the performance of weak classifiers by iteratively reweighting the training data to focus on misclassified points. AdaBoost combines many weak classifiers (shallow decision trees), adjusting their weights at each step. The final classifier aggregates all of the weak learners through weighted majority voting. Before training the model, I will tune its hyperparameters with GridSearchCV to find the best number of estimators and learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "325a8f39-ee65-4d81-bb25-46adfc8733af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 1, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "base_estimator = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\n",
    "ab = AdaBoostClassifier(estimator=base_estimator)\n",
    "param_grid = {'n_estimators': [50, 100, 150, 250],   \n",
    "              'learning_rate': [0.01, 0.1, 0.5, 1]}   \n",
    "grid_search = GridSearchCV(\n",
    "    estimator = ab,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,\n",
    "    scoring = 'f1_macro',\n",
    "    n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1da98ec2-c55d-43fa-a0f3-5898f3bd99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(n_estimators = 100, \n",
    "                        learning_rate = 0.1)\n",
    "ab.fit(X_train, y_train)\n",
    "ab_preds = ab.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e62f1-bb0b-4a30-ac66-c64a62ebc3db",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forests is an algorithm that uses bagging (bootstrap aggregating): training many individual decision trees on different random subsets of the data (bootstrapped samples) and random subsets of features. Each tree makes its own prediction, and the final classification is determined by majority vote across all trees. I am again using GridSearchCV to tune the key hyperparameters of the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaed049-8d85-45ae-89f8-8427d68b46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [200, 300, 400],      \n",
    "              'max_depth': [None, 10, 15, 20],             \n",
    "              'min_samples_split': [2, 5, 10],     \n",
    "              'min_samples_leaf': [1, 2, 4], \n",
    "              'max_features': ['sqrt', 'log2', None]}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = rf,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,\n",
    "    scoring = 'f1_macro',\n",
    "    n_jobs = -1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b44a2-8b1d-4ab5-87a6-d635ba278da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200, \n",
    "                            max_depth = 20, \n",
    "                            min_samples_split = 10, \n",
    "                            min_samples_leaf = 1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f5545-1754-4044-a305-2954bd2c4bc2",
   "metadata": {},
   "source": [
    "## Model Evaluations\n",
    "\n",
    "I'm going to evaluate and compare the performaces of the 3 classifiers using generalization error and macro F1. \n",
    "\n",
    "Generalization error quantifies how well the model will classify unseen data. It is computed as the accuracy score of the model predictions for the held-out testing set. While accuracy provides some measure of model performance, if the classes are imbalanced as they are in this case, accuracy can be misleading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0acace-cada-4e55-9c68-c8db2729bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa25fc-7112-4666-9b72-e17800205b6d",
   "metadata": {},
   "source": [
    "Our dataset has more than 3x as many normal health diagnosis examples as suspect and pathological examples combined, so a poor model could classifier every new case as healthy and still have accuracy over 75% despite also having a false negative rate of 100%. \n",
    "\n",
    "A metric which accounts for unbalanced classes is the macro F1-score. Generally, an F1-score balances precision and recall to punish models that either miss too many true cases (low recall) or produce too many false positives (low precision). A high F1-score indicates a model is good at classifying true cases without causing too many false positives. A macro F1-score is used for multi-class cases because it computes the F1-score separately for each class and then averages those three scores. This method works better than accuracy for imbalanced classes because it treats all classes equally, regardless of how many examples they have. In this case, macro F1 will emphasize how well the model identifies suspect and pathological fetuses, not just the majority healthy class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d09d14-2696-4576-875c-c28a59645519",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = np.array([metrics.accuracy_score(y_test, dt_preds), \n",
    "                 metrics.accuracy_score(y_test, ab_preds), \n",
    "                 metrics.accuracy_score(y_test, rf_preds)])\n",
    "f1s = np.array([metrics.f1_score(y_test, dt_preds, average='macro'), \n",
    "                metrics.f1_score(y_test, ab_preds, average='macro'), \n",
    "                metrics.f1_score(y_test, rf_preds, average='macro')])\n",
    "models = np.array(['DecisionTree', 'AdaBoost', 'RandomForest'])\n",
    "df = pd.DataFrame({'Model': models,\n",
    "              'Accuracy': accs,\n",
    "              'F1_macro': f1s})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6a131-52f3-47fb-bb4e-77dd361a9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(3)  \n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.bar(x - width/2, df['Accuracy'], width, \n",
    "       label='Accuracy', color = 'darkblue')\n",
    "ax.bar(x + width/2, df['F1_macro'], width, \n",
    "       label='Macro F1', color = 'darkorange')\n",
    "ax.set_ylabel('Metric Value')\n",
    "ax.set_title('Model Comparisons with Accuracy and Macro F1')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7ec74-ac9e-4eb7-9de6-5b4910828b03",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "The Random Forest model achieved the best overall performance, with an accuracy of 0.927 and a macro F1-score of 0.861, outperforming both the single Decision Tree and AdaBoost. The Decision Tree also performed well (accuracy 0.908, F1_macro 0.833), demonstrating that a single interpretable tree can capture much of the structure in the CTG dataset. AdaBoost, on the other hand, performed significantly worse (accuracy 0.798, F1_macro 0.419), likely due to the small dataset size or sensitivity to noisy cases. \n",
    "\n",
    "These results highlight that ensembles can improve robustness: Random Forest reduces variance through bagging and generalizes better than a single tree. In terms of interpretability, the single Decision Tree is the easiest to visualize and explain, making it useful for clinical contexts, while Random Forest and AdaBoost are harder to interpret directly. However, both ensembles allow for analysis of feature importance, which can provide insight into the most influential CTG features for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e4130-b00a-4494-9559-944d6412ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame({'Feature': X_train.columns, \n",
    "                      'Importance': rf.feature_importances_})\n",
    "fi_df = fi_df.sort_values(by='Importance', ascending=True)\n",
    "plt.barh(fi_df['Feature'], fi_df['Importance'], color='darkgreen')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34f294-b84b-4062-b227-22051994f0fd",
   "metadata": {},
   "source": [
    "The Random Forest model identified several CTG features as particularly important for predicting fetal health. The most influential features were abnormal_short_term_variability, percentage_of_time_with_abnormal_long_term_variability, and mean_value_of_short_term_variability, indicating that both short-term and long-term variability metrics are key indicators of fetal well-being. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4503743-d81c-4b6d-a655-c9a820e3148f",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Altogether, the Random Forest model not only demonstrated the highest predictive performance among the three classifiers but also provides a valuable tool for real-world fetal health assessment. By accurately identifying cases as normal, suspect, or pathological, the model could assist obstetricians in prioritizing high-risk pregnancies and supporting timely clinical interventions. The ability to quantify feature importance further enhances its practical utility, allowing clinicians to understand which aspects of a CTG most strongly influence predictions. While not as immediately interpretable as a single Decision Tree, the combination of high accuracy, robustness to variance, and insight into key features makes Random Forest a promising model for aiding decision-making in prenatal care."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
